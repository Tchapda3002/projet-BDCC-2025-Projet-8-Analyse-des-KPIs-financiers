# üìä Pipeline ELT BigQuery avec Streamlit

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)
![GCP](https://img.shields.io/badge/GCP-BigQuery-yellow.svg)

> Pipeline ETL moderne avec interface Streamlit pour l'extraction, le chargement et la transformation de donn√©es vers BigQuery

## üë§ Auteurs

- **TCHAPDA KOUADJO Wilfred Rod**
- **Pape Magette DIOP**
- **Fatou Soumaya WADE**
- **Naba Ahmadou Seydou TOURE**

## üéØ Vue d'ensemble

Ce projet impl√©mente un pipeline ELT (Extract, Load, Transform) complet avec une interface web moderne d√©velopp√©e en Streamlit. Il permet d'automatiser le flux de donn√©es depuis des sources externes (Data.gouv, INPI) vers Google BigQuery, en passant par Google Cloud Storage pour en suite permettre une visualisation des KPI financiers sur looker studio.

### ‚ú® Fonctionnalit√©s principales

- **Extraction automatis√©e** - T√©l√©chargement des donn√©es depuis Data.gouv et l'INPI
- **Stockage Cloud** - Conversion en format Parquet et stockage dans GCS
- **Chargement BigQuery** - Import automatique dans des tables BigQuery
- **Transformation SQL** - Cr√©ation de vues nettoy√©es et enrichies
- **Interface moderne** - Dashboard Streamlit 
- **Visualisation** - Int√©gration avec Looker Studio

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Sources de Donn√©es                        ‚îÇ
‚îÇ              (Data.gouv, INPI, APIs)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  1. EXTRACTION                               ‚îÇ
‚îÇ    ‚Ä¢ T√©l√©chargement HTTP/API                                ‚îÇ
‚îÇ    ‚Ä¢ Conversion Parquet                                      ‚îÇ
‚îÇ    ‚Ä¢ Horodatage des batchs                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Google Cloud Storage (GCS)                      ‚îÇ
‚îÇ    gs://bucket/raw_data/table__timestamp.parquet            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  2. CHARGEMENT                               ‚îÇ
‚îÇ    ‚Ä¢ Import BigQuery                                         ‚îÇ
‚îÇ    ‚Ä¢ Tables raw avec timestamp                               ‚îÇ
‚îÇ    ‚Ä¢ Gestion des sch√©mas                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 BigQuery - Tables Raw                        ‚îÇ
‚îÇ    ‚Ä¢ ratios_inpi_raw                                        ‚îÇ
‚îÇ    ‚Ä¢ stock_entreprises_raw                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  3. TRANSFORMATION                           ‚îÇ
‚îÇ    ‚Ä¢ Nettoyage SQL                                          ‚îÇ
‚îÇ    ‚Ä¢ Enrichissement                                          ‚îÇ
‚îÇ    ‚Ä¢ Cr√©ation de vues                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              BigQuery - Vues Transform√©es                    ‚îÇ
‚îÇ    ‚Ä¢ v_ratios_cleaned                                       ‚îÇ
‚îÇ    ‚Ä¢ v_stock_cleaned                                        ‚îÇ
‚îÇ    ‚Ä¢ v_looker_studio (vue finale)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Looker Studio                              ‚îÇ
‚îÇ              Dashboards & Visualisations                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üì¶ Structure du Projet

```
pipeline-etl-streamlit/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml              # Configuration principale
‚îÇ   ‚îî‚îÄ‚îÄ gcp-credentials.json     # Cl√©s GCP (non versionn√©)
‚îÇ
‚îú‚îÄ‚îÄ functions/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ step1_download.py        # Extraction des donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ step2_load.py            # Chargement BigQuery
‚îÇ   ‚îú‚îÄ‚îÄ step3_transform.py       # Transformation SQL
‚îÇ   ‚îî‚îÄ‚îÄ orchestrator.py          # Orchestration du pipeline
‚îÇ
‚îú‚îÄ‚îÄ interface/
‚îÇ   ‚îú‚îÄ‚îÄapp.py                    # Application web Streamlit
‚îÇ   ‚îî‚îÄ‚îÄ .streamlit/
‚îÇ        ‚îú‚îÄ‚îÄ config.toml
‚îÇ        ‚îú‚îÄ‚îÄ secrets.toml
‚îÇ           
‚îú‚îÄ‚îÄ venv/                        # Environnement virtuel
‚îú‚îÄ‚îÄ .env                         # Variables d'environnement
‚îú‚îÄ‚îÄ requirements.txt             # D√©pendances Python
‚îú‚îÄ‚îÄ README.md                    # Documentation
‚îú‚îÄ‚îÄ Dockerfile.streamlit         # Conteneur pour l'application streamlit
‚îî‚îÄ‚îÄ .gitignore                   # Fichiers √† ignorer
```

## üöÄ Installation

### Pr√©requis

- Python 3.9+
- Compte Google Cloud Platform avec :
  - BigQuery API activ√©e
  - Cloud Storage API activ√©e
  - Compte de service avec permissions appropri√©es
- Git

### Installation locale

1. **Cloner le repository**

```bash
git clone https://github.com/Tchapda3002/pipeline-etl-streamlit.git
cd pipeline-etl-streamlit
```

2. **Cr√©er un environnement virtuel**

```bash
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
```

3. **Installer les d√©pendances**

```bash
pip install -r requirements.txt
```

4. **Configuration GCP**

Cr√©er le fichier `config/gcp-credentials.json` avec vos credentials GCP :

```json
{
  "type": "service_account",
  "project_id": "votre-projet-id",
  "private_key_id": "...",
  "private_key": "...",
  "client_email": "...",
  "client_id": "...",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "..."
}
```

5. **Configurer le fichier config.yaml**

```yaml
gcp:
  project_id: "votre-projet-id"
  region: "europe-west1"
  credentials_path: "config/gcp-credentials.json"

storage:
  bucket_name: "votre-bucket"
  raw_folder: "raw_data"

bigquery:
  dataset: "votre_dataset"

data_sources:
  sources:
    - name: "Stock entreprises"
      url: "https://data.gouv.fr/..."
      active: true
      description: "Donn√©es d'entreprises"
    # Ajouter vos sources ici
```

6. **Lancer l'application**

```bash
streamlit run interface/streamlit_app.py
```

L'application sera accessible √† `http://localhost:8501`

## ‚òÅÔ∏è D√©ploiement sur Streamlit Cloud

### Configuration des secrets

1. Aller sur [Streamlit Cloud](https://share.streamlit.io/)
2. Connecter votre repository GitHub
3. Dans **Advanced settings** > **Secrets**, ajouter :

```toml
[gcp]
project_id = "votre-projet-id"
type = "service_account"
private_key_id = "..."
private_key = "-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n"
client_email = "..."
client_id = "..."
auth_uri = "https://accounts.google.com/o/oauth2/auth"
token_uri = "https://oauth2.googleapis.com/token"
auth_provider_x509_cert_url = "https://www.googleapis.com/oauth2/v1/certs"
client_x509_cert_url = "..."
```

### D√©ploiement

```bash
git add .
git commit -m "Configuration d√©ploiement"
git push origin main
```

Streamlit Cloud d√©ploiera automatiquement votre application.

## üéÆ Utilisation

### Ligne de commande (CLI)

Vous pouvez ex√©cuter chaque √©tape du pipeline directement depuis le terminal :

#### **√âtape 1 : Extraction**

```bash
# Extraire toutes les sources
python -m functions.step1_download

# Extraire une source sp√©cifique
python -m functions.step1_download --source "Stock entreprises"
```

#### **√âtape 2 : Chargement**

```bash
# Lister les batchs disponibles dans Cloud Storage
python -m functions.step2_load list

# Charger le batch le plus r√©cent (par d√©faut)
python -m functions.step2_load

# Charger un timestamp sp√©cifique
python -m functions.step2_load --timestamp "20241210_14-30-00"
```

#### **√âtape 3 : Transformation**

```bash
# Lister les timestamps disponibles dans BigQuery
python -m functions.step3_transform list

# Cr√©er les vues avec le timestamp le plus r√©cent (par d√©faut)
python -m functions.step3_transform

# Cr√©er les vues avec un timestamp sp√©cifique
python -m functions.step3_transform --timestamp "2024-12-10T14:30:00"
```

#### **Pipeline complet**

```bash
# Ex√©cuter tout le pipeline (extraction ‚Üí chargement ‚Üí transformation)
python -m functions.orchestrator

# Ignorer certaines √©tapes
python -m functions.orchestrator --skip-download  # Ignorer l'extraction
python -m functions.orchestrator --skip-load      # Ignorer le chargement

# Extraire une source sp√©cifique
python -m functions.orchestrator --source "Stock entreprises"
```

### Interface Streamlit

L'application propose 5 onglets principaux :

#### 1. **Accueil**
- Vue d'ensemble des m√©triques
- Nombre de batchs disponibles
- Statistiques BigQuery
- Historique des timestamps

#### 2. **Extraction**
- S√©lection de la source de donn√©es
- T√©l√©chargement et conversion en Parquet
- Logs en temps r√©el
- Bouton d'arr√™t

#### 3. **Chargement**
- S√©lection du batch √† charger
- Import vers BigQuery
- Tables raw cr√©√©es automatiquement
- Suivi de l'ex√©cution

#### 4. **Transformation**
- S√©lection du timestamp
- Cr√©ation des vues SQL
- Nettoyage et enrichissement
- Lien vers Looker Studio

#### 5. **Pipeline Complet**
- Ex√©cution des 3 √©tapes en s√©quence
- Options d'ignorer certaines √©tapes
- Logs en temps r√©el
- Dur√©e totale d'ex√©cution

### Exemple d'utilisation

**Workflow recommand√© pour d√©butants :**

**Via l'interface Streamlit :**
```bash
1. Aller dans "Pipeline Complet"
2. Cliquer sur "Lancer le pipeline"
3. Attendre 5-10 minutes
4. Consulter le tableau de bord Looker Studio
```

**Via la ligne de commande :**
```bash
# Pipeline complet en une commande
python -m functions.orchestrator
```

**Workflow pas √† pas :**

**Via l'interface Streamlit :**
```bash
1. Extraction ‚Üí T√©l√©charger les donn√©es
2. Chargement ‚Üí Importer dans BigQuery
3. Transformation ‚Üí Cr√©er les vues nettoy√©es
4. Visualisation ‚Üí Ouvrir Looker Studio
```

**Via la ligne de commande :**
```bash
# √âtape 1 : Extraction
python -m functions.step1_download

# √âtape 2 : Lister les batchs disponibles
python -m functions.step2_load list

# Charger le batch le plus r√©cent
python -m functions.step2_load

# √âtape 3 : Lister les timestamps BigQuery
python -m functions.step3_transform list

# Cr√©er les vues avec le timestamp le plus r√©cent
python -m functions.step3_transform

# Ou tout en une fois
python -m functions.orchestrator
```

**Cas d'usage avanc√©s :**

```bash
# Extraire uniquement les donn√©es INPI
python -m functions.step1_download --source "Ratios INPI"

# Charger un batch sp√©cifique
python -m functions.step2_load --timestamp "20241210_09-15-00"

# Cr√©er les vues pour un timestamp sp√©cifique
python -m functions.step3_transform --timestamp "2024-12-10T09:15:00"

# Pipeline sans extraction (donn√©es d√©j√† t√©l√©charg√©es)
python -m functions.orchestrator --skip-download

# Pipeline sans chargement (donn√©es d√©j√† dans BigQuery)
python -m functions.orchestrator --skip-load

# Pipeline avec une source sp√©cifique
python -m functions.orchestrator --source "Stock entreprises"
```

## üîß Configuration GCP

### Permissions requises

Le compte de service GCP doit avoir les r√¥les suivants :

- **Storage Object Admin** - Pour cr√©er/lire les objets dans GCS
- **BigQuery Data Editor** - Pour cr√©er des tables et vues
- **BigQuery Job User** - Pour ex√©cuter des requ√™tes

### Commandes gcloud utiles

```bash
# Cr√©er un bucket
gsutil mb -p votre-projet-id -l europe-west1 gs://votre-bucket/

# Cr√©er un dataset BigQuery
bq mk --dataset votre-projet-id:votre_dataset

# Lister les tables
bq ls votre-projet-id:votre_dataset

# V√©rifier les permissions
gcloud projects get-iam-policy votre-projet-id
```

## üìä Sources de donn√©es

Le pipeline peut extraire des donn√©es depuis :

- **Data.gouv.fr** - Donn√©es ouvertes du gouvernement fran√ßais
- **INPI** - Donn√©es de l'Institut National de la Propri√©t√© Industrielle
- **APIs personnalis√©es** - Ajout de nouvelles sources via configuration

### Ajouter une nouvelle source

Dans `config/config.yaml` :

```yaml
data_sources:
  sources:
    - name: "Ma nouvelle source"
      url: "https://api.example.com/data"
      active: true
      description: "Description de la source"
      format: "csv"  # ou json, parquet, etc.
```

## üîß R√©f√©rence CLI

### Commandes disponibles

#### **step1_download** - Extraction des donn√©es

```bash
# Syntaxe
python -m functions.step1_download [--source SOURCE_NAME]

# Arguments
--source    Nom de la source √† extraire (optionnel)
            Si non sp√©cifi√©, toutes les sources actives sont extraites

# Exemples
python -m functions.step1_download                           # Toutes les sources
python -m functions.step1_download --source "Stock entreprises"
python -m functions.step1_download --source "Ratios INPI"
```

**Sortie :**
- Fichiers Parquet dans Cloud Storage : `gs://bucket/raw_data/table__timestamp.parquet`
- Format timestamp : `YYYYMMDD_HH-MM-SS`

#### **step2_load** - Chargement BigQuery

```bash
# Syntaxe
python -m functions.step2_load [list|load] [--timestamp TIMESTAMP]

# Commandes
list                Affiche tous les batchs disponibles dans GCS
load (d√©faut)       Charge un batch dans BigQuery

# Arguments
--timestamp    Timestamp du batch √† charger (optionnel)
               Format: YYYYMMDD_HH-MM-SS
               Si non sp√©cifi√©, le batch le plus r√©cent est utilis√©

# Exemples
python -m functions.step2_load list                           # Liste les batchs
python -m functions.step2_load                                # Charge le plus r√©cent
python -m functions.step2_load --timestamp "20241210_14-30-00"
```

**Sortie :**
- Tables BigQuery cr√©√©es/remplac√©es :
  - `projet.dataset.ratios_inpi_raw`
  - `projet.dataset.stock_entreprises_raw`
- Colonne `extraction_timestamp` ajout√©e automatiquement

#### **step3_transform** - Transformation SQL

```bash
# Syntaxe
python -m functions.step3_transform [list|transform] [--timestamp TIMESTAMP]

# Commandes
list                  Affiche tous les timestamps disponibles dans BigQuery
transform (d√©faut)    Cr√©e les vues transform√©es

# Arguments
--timestamp    Timestamp pour filtrer les vues (optionnel)
               Format: YYYY-MM-DDTHH:MM:SS (ISO 8601)
               Si non sp√©cifi√©, le timestamp le plus r√©cent est utilis√©

# Exemples
python -m functions.step3_transform list                      # Liste les timestamps
python -m functions.step3_transform                           # Transforme le plus r√©cent
python -m functions.step3_transform --timestamp "2024-12-10T14:30:00"
```

**Sortie :**
- Vues BigQuery cr√©√©es/remplac√©es :
  - `projet.dataset.v_ratios_cleaned`
  - `projet.dataset.v_stock_cleaned`
  - `projet.dataset.v_looker_studio`

#### **orchestrator** - Pipeline complet

```bash
# Syntaxe
python -m functions.orchestrator [OPTIONS]

# Arguments
--source          Nom de la source pour l'extraction (optionnel)
--skip-download   Ignore l'√©tape d'extraction
--skip-load       Ignore l'√©tape de chargement

# Exemples
python -m functions.orchestrator                              # Pipeline complet
python -m functions.orchestrator --skip-download              # Sans extraction
python -m functions.orchestrator --skip-load                  # Sans chargement
python -m functions.orchestrator --source "Stock entreprises"
python -m functions.orchestrator --skip-download --skip-load  # Transformation uniquement
```

**Sortie :**
- Ex√©cution s√©quentielle : Extraction ‚Üí Chargement ‚Üí Transformation
- Rapport de succ√®s/√©chec pour chaque √©tape
- Dur√©e totale d'ex√©cution

### Sc√©narios d'utilisation CLI

#### **Automatisation avec cron**

```bash
# Ajouter au crontab (crontab -e)

# Ex√©cution quotidienne √† 2h du matin
0 2 * * * cd /chemin/vers/projet && /chemin/vers/venv/bin/python -m functions.orchestrator

# Ex√©cution toutes les 6 heures
0 */6 * * * cd /chemin/vers/projet && /chemin/vers/venv/bin/python -m functions.orchestrator --skip-download
```

#### **Scripts bash**

```bash
#!/bin/bash
# pipeline.sh - Script d'automatisation

set -e  # Arr√™ter en cas d'erreur

echo "D√©marrage du pipeline ETL..."

# Activer l'environnement virtuel
source venv/bin/activate

# Extraction
echo "√âtape 1/3 : Extraction..."
python -m functions.step1_download

# Chargement
echo "√âtape 2/3 : Chargement..."
python -m functions.step2_load

# Transformation
echo "√âtape 3/3 : Transformation..."
python -m functions.step3_transform

echo "Pipeline termin√© avec succ√®s !"
```

#### **Int√©gration CI/CD**

```yaml
# .github/workflows/etl-pipeline.yml
name: ETL Pipeline

on:
  schedule:
    - cron: '0 2 * * *'  # Tous les jours √† 2h
  workflow_dispatch:      # D√©clenchement manuel

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Run pipeline
        env:
          GCP_CREDENTIALS: ${{ secrets.GCP_CREDENTIALS }}
        run: |
          echo "$GCP_CREDENTIALS" > config/gcp-credentials.json
          python -m functions.orchestrator
```

## üêõ D√©pannage

### Probl√®mes courants

#### Erreur : "Impossible de se connecter √† GCP"

**Solution :**
- V√©rifier que `gcp-credentials.json` existe
- V√©rifier les permissions du compte de service
- V√©rifier le `project_id` dans config.yaml

#### Erreur : "Aucun batch disponible"

**Solution :**
- Lancer d'abord l'extraction
- V√©rifier que les fichiers sont dans GCS
- V√©rifier le pr√©fixe `raw_folder` dans config.yaml

#### Le pipeline est lent

**Solution :**
- Utiliser "Ignorer l'extraction" si les donn√©es sont d√©j√† t√©l√©charg√©es
- Utiliser "Ignorer le chargement" si les donn√©es sont d√©j√† dans BigQuery
- V√©rifier la connexion internet

### Logs et debugging

Pour activer les logs d√©taill√©s :

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

Inspecter les donn√©es dans BigQuery :

```sql
-- V√©rifier les donn√©es raw
SELECT * FROM `projet.dataset.ratios_inpi_raw` LIMIT 10;

-- V√©rifier les timestamps disponibles
SELECT DISTINCT extraction_timestamp 
FROM `projet.dataset.ratios_inpi_raw`
ORDER BY extraction_timestamp DESC;

-- V√©rifier les vues transform√©es
SELECT COUNT(*) FROM `projet.dataset.v_looker_studio`;
```

## üîê S√©curit√©

### Bonnes pratiques

- ‚ùå **Ne jamais** commiter `gcp-credentials.json`
- ‚úÖ Utiliser `.gitignore` pour exclure les fichiers sensibles
- ‚úÖ Utiliser des secrets Streamlit Cloud pour le d√©ploiement
- ‚úÖ Limiter les permissions du compte de service au strict minimum
- ‚úÖ Activer l'authentification sur Streamlit Cloud si n√©cessaire

### Fichiers √† ne pas versionner

```gitignore
# Credentials
config/gcp-credentials.json
.streamlit/secrets.toml

# Environment
.env
venv/
__pycache__/

# Data
data/
*.parquet
*.csv
```


- Repository: [pipeline-etl-streamlit](https://github.com/Tchapda3002/pipeline-etl-streamlit)

## üôè Remerciements

- Madame Mously DIAW, Enseignante de Big Data et Clous Computing
- [Streamlit](https://streamlit.io/) - Framework web Python
- [Google Cloud Platform](https://cloud.google.com/) - Infrastructure cloud
- [Data.gouv.fr](https://data.gouv.fr/) - Donn√©es ouvertes fran√ßaises
- [INPI](https://www.inpi.fr/) - Donn√©es propri√©t√© industrielle

## üìö Ressources

### Documentation

- [Documentation Streamlit](https://docs.streamlit.io/)
- [BigQuery Documentation](https://cloud.google.com/bigquery/docs)
- [Cloud Storage Documentation](https://cloud.google.com/storage/docs)
- [Looker Studio](https://lookerstudio.google.com/)

### Tutoriels

- [Getting Started with BigQuery](https://cloud.google.com/bigquery/docs/quickstarts)
- [Streamlit Tutorial](https://docs.streamlit.io/get-started/tutorials)
- [GCP Best Practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations)

---

